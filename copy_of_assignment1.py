# -*- coding: utf-8 -*-
"""Copy of Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19zStAPCQ_mdK_McwLl7ijQXUoszZp-LN

#ğŸš€ Assignment 1: Predicting Vehicle Fuel Efficiency

**Objective**: Use your new Neural Network skills to predict a car's fuel efficiency (MPG) based on its characteristics (Horsepower, Weight, etc.)
##ğŸ› ï¸ Step 0: Setup & Data Loading

We will use the famous "Auto MPG" dataset.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load dataset from Seaborn
df = sns.load_dataset('mpg')

# Quick look at the data
print(df.head())

# Cleaning: Remove rows with missing values
df = df.dropna()

# We want to predict 'mpg' using these features:
features = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']
X = df[features]
y = df['mpg']

print("\nâœ… Data Loaded and Cleaned!")

"""##ğŸ§¹ Step 1: Pre-processing (The "Cleaning" Phase)

Neural Networks are very sensitive to the scale of numbers. If one column is 2000 (Weight) and another is 15 (Acceleration), the math gets messy. We need to **Normalize** the data.

###ğŸ“ Your Task:

Split the data into Training and Testing sets, then scale them.
"""

# 1. Split the data (80% Train, 20% Test)
# TODO: see the import code
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Scaling (Normalization)
scaler = StandardScaler()

# TODO: Fit the scaler on training data and transform both sets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… Data Pre-processed!")

"""##ğŸ§  Step 2: Build Your Brain (Neural Network)

Now, design a Neural Network. Since this is a *Regression** problem (predicting a continuous number), your last layer should have 1 neuron with no activation.

###ğŸ“ Your Task:

Build a model with at least two hidden layers.
"""

# TODO: Define the Sequential model
model = Sequential([
    # Layer 1: Try 64 neurons with 'relu' activation
    Dense(64, activation='relu', input_shape=(len(features),)),

    # Layer 2: Try 32 neurons with 'relu' activation
    # Output Layer: 1 neuron

    ### Your code here
    Dense(32,activation='relu',input_shape=(len(features),)),
    Dense(1),
])

# TODO: Compile the model
### Your code here
model.compile(
    optimizer='adam',
    loss='mean_squared_error',
    metrics=['mae']
)


print("âœ… Model Constructed!")
model.summary()

"""##ğŸš€ Step 3: Training the Model

###ğŸ“ Your Task:

Train the model for 100 epochs.
"""

# TODO: Train the model using .fit()
# Remember to use the SCALED features (X_train_scaled)
history = model.fit(
  X_train,y_train,
    epochs=100,
    validation_split=0.2,
    verbose=1
)

print("âœ… Training Complete!")

"""##ğŸ“Š Step 4: Evaluation

Let's see if your AI is actually a good "Engineer."

###ğŸ“ Your Task:

Generate predictions and calculate the scores.
"""

predictions = model.predict(X_test)
r2 = r2_score(y_test, predictions)
mae = mean_absolute_error(y_test, predictions)

print(f"Final R2 Score: {r2:.4f}")
print(f"Mean Absolute Error: {mae:.2f} MPG")

# Visualization: Predicted vs Actual
plt.figure(figsize=(8,6))
plt.scatter(y_test, predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual MPG')
plt.ylabel('Predicted MPG')
plt.title('How well did we do?')
plt.show()

"""##ğŸŒŸ Bonus Challenge (For Sophies / Fast Learners)

1. **The "Brain Size" Test:** Go back and change the number of neurons to 128. Does the R2 score go up or down?

2. **The "Overfitting" Check:** Look at your training history. If your validation loss is much higher than your training loss, your model is memorizing! Try adding a Dropout layer.

Created with â¤ï¸ for WiDS Project
"""