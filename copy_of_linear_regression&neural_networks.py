# -*- coding: utf-8 -*-
"""Copy of Linear_Regression&Neural_Networks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iO7jy7hB1uTZgQrDuTX2DcvK2FjATvg0

# üß† Week 3: From Straight Lines to Brain Power
Welcome to Week 3! Today, we are going to see how computers learn patterns. We'll start with the simplest math possible (Linear Regression) and see exactly where it breaks. Then, we'll "upgrade" our computer's brain to a Neural Network.
##üõ†Ô∏è Step 0: Setting the Stage
Don't worry about the code in this block! This is just me bringing in the toolboxes (Libraries) we need to make things look pretty and generate our data.

**Just hit the "Play" button below!**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import logging
tf.get_logger().setLevel(logging.ERROR)

print("‚úÖ Environment Ready! Let's build some AI.")

"""#üìè Part 1: Linear Regression (The "Straight Line" Logic)

Imagine you are trying to predict the price of a house based on its square footage. Usually, as the area goes up, the price goes up in a straight line. This is Linear Regression.

Formula: $y = mx + c$

**üìù Your Task:**

I have generated some "House Data" for you below. Your job is to tell the computer to find the "Line of Best Fit."
"""

# 1. Generating synthetic data
np.random.seed(42)
house_size = np.array([500, 700, 1000, 1200, 1500, 1800, 2000, 2500]).reshape(-1, 1)
house_price = np.array([50, 75, 110, 130, 165, 190, 210, 260]) # in Lakhs

# 2. Setup the Model
# TODO: Create a LinearRegression object named 'model'
# Hint: see above code block

model = LinearRegression()# YOUR CODE HERE

# 3. Train the Model
# TODO: Use the .fit() function with house_size and house_price

# YOUR CODE HERE
model.fit(house_size,house_price)
# 4. Predict
predictions = model.predict(house_size)

# 5. Metrics
print(f"R2 Score: {r2_score(house_price, predictions):.4f}")
print(f"Mean Absolute Error: {mean_absolute_error(house_price, predictions):.2f} Lakhs")

# Visualization
plt.scatter(house_size, house_price, color='blue', label='Actual Data')
plt.plot(house_size, predictions, color='red', label='Linear Regression Line')
plt.xlabel('Size (sq ft)')
plt.ylabel('Price (Lakhs)')
plt.legend()
plt.show()

print(f"Prediction for a 1600 sq ft house: {model.predict([[1600]])[0]:.2f} Lakhs")

"""##üõë The "Wait, it Broke!" Moment

Linear Regression is great for straight lines. But what if the world isn't straight?

Imagine our Self-Driving Car is looking at a curved road or a complex traffic sign pattern. Let's create some "Curvy Data" and see if our Linear Regression can handle it.
"""

# Generating "Curvy" Data
X_curvy = np.linspace(-10, 10, 100).reshape(-1, 1)
y_curvy = X_curvy**2 + np.random.normal(0, 10, X_curvy.shape) # A parabola (U-shape)

# Trying Linear Regression again
bad_model = LinearRegression().fit(X_curvy, y_curvy)
bad_pred = bad_model.predict(X_curvy)

# Metrics for the "Bad" Model
print(f"R2 Score (Linear on Curvy): {r2_score(y_curvy, bad_pred):.4f}")
print(f"MAE (Linear on Curvy): {mean_absolute_error(y_curvy, bad_pred):.2f}")

# Visualization
plt.scatter(X_curvy, y_curvy, color='gray', alpha=0.5, label='Complex Data')
plt.plot(X_curvy, bad_pred, color='red', linewidth=3, label='Linear Regression')
plt.title("Linear Regression is struggling!")
plt.legend()
plt.show()

"""**Observation:** Look at the Red Line. It's perfectly straight, but the data is a "U" shape! Linear Regression is "too simple" to understand curves. This is called **Underfitting**.

#üß† Part 2: Neural Networks (The "Bending" Logic)
To solve the curve problem, we need a **Neural Network**. Think of it as a series of connected "Neurons" that can bend and twist a line until it fits the data perfectly.

We will use Hidden Layers and an Activation Function (**ReLU**) to give the model "flexibility."

###**üìù Your Task:**

Let's build a simple Brain (Neural Network) using Keras.
"""

# 1. Build the Brain
# We want: 1 Input -> 64 Neurons -> 64 Neurons -> 1 Output
nn_model = Sequential([
    # TODO: Add a Dense layer with 64 neurons, relu activation, and input_shape (1,)
    Dense(64, activation='relu', input_shape=(1,)),

    # TODO: Add another Dense layer with 64 neurons and relu activation
    Dense(64, activation='relu'),

    # TODO: Add the final Output Layer (just 1 neuron for the prediction)
    Dense(1)
])

# 2. Compile (Tell the AI how to learn)
# Hint: Use adam optimizer and mse loss (Mean Squared Error)
nn_model.compile(optimizer='adam', loss='mse')

# 3. Train the Brain
print("Training the Neural Network... please wait.")
# TODO: Train for 200 epochs
nn_model.fit(X_curvy, y_curvy, epochs=200, verbose=0)

# 4. Predict
nn_pred = nn_model.predict(X_curvy)

# 5. Metrics for the Neural Network
print(f"R2 Score (Neural Network): {r2_score(y_curvy, nn_pred):.4f}")
print(f"MAE (Neural Network): {mean_absolute_error(y_curvy, nn_pred):.2f}")

# Visualization
plt.scatter(X_curvy, y_curvy, color='gray', alpha=0.5, label='Complex Data')
plt.plot(X_curvy, nn_pred, color='green', linewidth=3, label='Neural Network')
plt.title("Neural Network fits the curve!")
plt.legend()
plt.show()

"""###**üí° Conclusion: Why does this matter for our Project?**

1. **Linear Regression** is like a rigid ruler. It can't understand complex shapes.
2. **Neural Networks** are like clay. They can be molded into any shape to fit the data.
3. **Traffic Signs** are complex. A "Stop" sign has 8 edges, colors, and textures. A straight line can never identify a Stop sign, but a Neural Network can!

**Next Week**: We will see a special type of Neural Network called a **CNN** that is specifically designed to "see" images!

Created with ‚ù§Ô∏è for WiDS Project
"""